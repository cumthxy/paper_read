
## xGen-MM (BLIP-3): A Family of Open Large Multimodal Models
[xGen-MM (BLIP-3): 一个开放的大型多模态模型家族](https://arxiv.org/abs/2408.08872)
本报告介绍 xGen-MM（亦称 BLIP-3），这是一个用于开发大型多模态模型（LMMs）的框架。该框架整合了精心挑选的数据集、训练方案、模型架构及一系列 LMMs。作为 xGen-MultiModal 的简称，xGen-MM 延续了 Salesforce xGen 在基础 AI 模型领域的工作。我们的模型在多种任务上，包括单图像与多图像基准测试，均接受了严格评估。预训练的基础模型展现出卓越的上下文学习能力，而经过指令优化的模型在同等规模的开放源代码 LMMs 中表现出色。此外，我们还推出了一款经 DPO 安全优化的模型，旨在减少如幻觉等不良行为，并增强安全性。我们公开了模型、大规模精选数据集及微调代码库，以推动 LMM 研究的发展。相关资源将在项目页面上提供。



## JPEG-LM: LLMs as Image Generators with Canonical Codec Representations
[JPEG-LM: 利用标准编解码器表示的大语言模型进行图像生成](https://arxiv.org/abs/2408.08459)
近期，图像和视频生成领域开始采用自回归大语言模型架构，因其通用性和易于集成到多模态系统的潜力。将自回归训练从语言生成应用到视觉生成的关键在于离散化——将连续的图像和视频数据转换为离散的Token。常见的离散化方法包括直接建模原始像素值（过于冗长）或采用向量量化（需要复杂的预训练）。本研究提出了一种新方法，即直接将图像和视频建模为通过标准编解码器（如JPEG、AVC/H.264）压缩的文件。我们使用未经视觉特定修改的默认Llama架构，从零开始预训练JPEG-LM生成图像（并作为概念验证，训练AVC-LM生成视频），通过直接输出JPEG和AVC格式的压缩文件字节。评估结果显示，这种简单直接的方法在图像生成上比基于像素的建模和复杂的向量量化基准更为有效，实现了31%的FID降低。分析表明，JPEG-LM在生成长尾视觉元素方面相比向量量化模型具有显著优势。总体而言，我们的研究表明，使用标准编解码器表示可以有效降低语言生成与视觉生成之间的技术壁垒，为未来多模态语言/图像/视频大语言模型的研究铺平道路。



## Automated Design of Agentic Systems
[自动化设计智能体系统](https://arxiv.org/abs/2408.08435)
研究人员正投入大量精力开发强大的通用智能体，其中基础模型被用作智能体系统（例如思维链、自我反思、Toolformer）的模块。然而，机器学习的历史告诉我们，手工设计的解决方案最终会被学习型解决方案所取代。我们提出一个新的研究领域——自动化设计智能体系统（ADAS），旨在自动创建强大的智能体系统设计，包括发明新的构建模块和/或以新的方式组合它们。我们进一步证明，在ADAS中存在一种未被探索但有前景的方法，即智能体可以通过代码定义，并且可以通过一个元智能体自动发现新的智能体，该元智能体在代码中编程出更好的智能体。鉴于编程语言是图灵完备的，这种方法理论上可以学习任何可能的智能体系统：包括新颖的提示、工具使用、控制流程及其组合。我们提出了一种简单而有效的算法，名为元智能体搜索，以展示这一理念，其中元智能体基于不断增长的先前发现档案迭代编程出有趣的新智能体。通过在多个领域（包括编码、科学和数学）进行广泛实验，我们展示了我们的算法可以逐步发明具有新颖设计的智能体，这些智能体大大优于最先进的手工设计智能体。重要的是，我们始终观察到一个令人惊讶的结果：由元智能体搜索发明的智能体即使在跨领域和模型转移时也能保持优越的性能，展示了它们的鲁棒性和通用性。如果我们能够安全地开发它，我们的工作展示了朝着自动设计越来越强大的智能体系统以造福人类这一激动人心的新研究方向的潜力。



## LongVILA: Scaling Long-Context Visual Language Models for Long Videos
[LongVILA: 扩展长视频上下文视觉语言模型](https://arxiv.org/abs/2408.10188)
长上下文处理能力对于多模态基础模型至关重要。我们推出了 LongVILA，这是一个全面的长上下文视觉语言模型解决方案，涵盖系统架构、模型训练和数据集构建。在系统层面，我们首创了多模态序列并行 (MM-SP) 系统，该系统支持长上下文的训练与推理，能在 256 个 GPU 上实现 200 万 Token 上下文长度的训练。MM-SP 系统效率显著，比环形序列并行快 2.1 至 5.7 倍，在纯文本环境下比 Megatron-LM 快 1.1 至 1.4 倍，并能与 Hugging Face 的 Transformers 框架无缝对接。在模型训练方面，我们设计了一个包含对齐、预训练、上下文扩展及长短联合监督微调的五阶段训练流程。数据集方面，我们精心打造了大规模视觉语言预训练数据集和长视频指令遵循数据集，以支撑多阶段训练需求。LongVILA 的全栈解决方案将 VILA 模型的处理帧数提升了 128 倍（从 8 帧增至 1024 帧），显著提高了长视频字幕的评分（从 2.00 提升至 3.26，即 1.6 倍），并在 1400 帧视频（上下文长度达 27.4 万 Token）中实现了 99.5% 的准确率。此外，LongVILA-8B 模型在 VideoMME 基准测试中，随着视频帧数的增加，其在长视频处理性能上展现出持续的提升。



## MeshFormer: High-Quality Mesh Generation with 3D-Guided Reconstruction Model
[MeshFormer: 利用3D引导重建模型生成高质量网格](https://arxiv.org/abs/2408.10198)
近期，开放世界场景下的3D重建模型受到了显著关注。然而，由于缺乏足够的3D归纳偏置，现有方法往往伴随着高昂的训练成本，且难以提取出高质量的3D网格。在本研究中，我们引入了MeshFormer，这是一种稀疏视角重建模型，它明确地利用了3D本征结构、输入引导以及训练监督。具体来说，我们摒弃了传统的三平面表示法，转而在3D稀疏体素中存储特征，并融合Transformer与3D卷积技术，以此来充分利用显式的3D结构和投影偏置。除了稀疏视角的RGB输入外，我们还要求网络处理输入并生成相应的法线图。这些输入的法线图可以由2D扩散模型预测生成，从而在几何学习的引导和细化过程中发挥重要作用。此外，通过将有符号距离函数（SDF）监督与表面渲染技术相结合，我们能够直接学习生成高质量的网格，而无需依赖复杂的多阶段训练流程。通过整合这些显式的3D偏置，MeshFormer能够实现高效训练，并输出具有精细几何细节的高质量纹理网格。它还能够与2D扩散模型无缝集成，从而支持快速执行单图像到3D以及文本到3D的转换任务。项目页面：https://meshformer3d.github.io



## TableBench: A Comprehensive and Complex Benchmark for Table Question Answering
[TableBench: 一个全面且复杂的表格问答基准测试](https://arxiv.org/abs/2408.09174)
大语言模型 (LLMs) 的最新进展极大地增强了其对表格数据的解析和处理能力，带来了前所未有的功能。尽管如此，LLMs 在工业应用中仍遇到重大挑战，尤其是由于处理真实世界表格数据所需的推理复杂性增加，这凸显了学术基准与实际应用之间的明显差距。为了弥合这一差距，我们对表格数据在工业场景中的应用进行了深入研究，并提出了一个全面且复杂的基准 TableBench，涵盖了四个主要类别下的 18 个领域的表格问答 (TableQA) 能力。此外，我们推出了 TableLLM，该模型在我们精心构建的训练集 TableInstruct 上训练，其性能可与 GPT-3.5 媲美。在 TableBench 上进行的大量实验显示，无论是开源还是专有的 LLMs，都有很大的提升空间以满足现实需求，其中最先进的模型 GPT-4 仅获得了与人类相比相对较低的分数。



## To Code, or Not To Code? Exploring Impact of Code in Pre-training
[编码与否？探索预训练中代码的影响](https://arxiv.org/abs/2408.10914)
在预训练数据混合中包含代码，即便对于非专门设计用于代码的模型，已成为大语言模型 (LLM) 预训练的常见做法。虽然业内普遍认为代码数据对通用大语言模型的性能至关重要，但关于代码对非代码任务的确切影响分析尚显不足。本研究旨在系统探讨代码数据对通用性能的影响，具体问题为：“预训练中使用的代码数据对大量下游任务（不仅仅是代码生成）有何影响？”我们进行了广泛的消融实验，并在一系列自然语言推理任务、世界知识任务、代码基准以及模型大小从470M到2.8B参数的LLM作为评判的胜率上进行了评估。结果显示，代码是泛化能力远超编码任务的关键构建块，且代码质量的改进对所有任务都有显著影响。具体而言，与仅使用文本进行预训练相比，增加代码使自然语言推理能力相对提升8.2%，世界知识理解提升4.2%，生成胜率提高6.6%，代码性能提升达12倍。这一发现表明，投资于代码质量及在预训练中保留代码具有积极意义。



## Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model
[Transfusion: 预测下一个 Token 并利用单一多模态模型进行图像扩散](https://arxiv.org/abs/2408.11039)
我们引入了 Transfusion，这是一种在离散和连续数据上训练多模态模型的方法。Transfusion 结合了语言模型损失函数（即下一个 Token 预测）与扩散过程，用于在混合模态序列上训练单个 Transformer。我们从零开始预训练了多个高达 70 亿参数的 Transfusion 模型，这些模型在文本和图像数据的混合上进行训练，并建立了关于多种单模态和跨模态基准的缩放定律。我们的实验表明，Transfusion 在缩放方面显著优于将图像量化并在离散图像 Token 上训练语言模型的方法。通过引入特定模态的编码和解码层，我们可以进一步提高 Transfusion 模型的性能，甚至可以将每个图像压缩到仅 16 个 Patch。我们还证明了，将我们的 Transfusion 方法扩展到 70 亿参数和 2 万亿多模态 Token 时，可以生成与类似规模的扩散模型和语言模型相媲美的图像和文本，从而同时获得两者的优势。



## TWLV-I: Analysis and Insights from Holistic Evaluation on Video Foundation Models
[TWLV-I: 视频基础模型的整体评估分析与洞察](https://arxiv.org/abs/2408.11318)
在这项研究中，我们探讨了如何以公平且稳健的方式评估视频基础模型。不同于语言或图像基础模型，视频基础模型的评估往往涉及不同的参数（如采样率、帧数、预训练步骤等），这使得进行公平且稳健的比较颇具挑战。为此，我们设计了一套精细的评估框架，旨在衡量视频理解的两大核心能力：外观理解和运动理解。研究结果显示，无论是采用文本监督的UMT和InternVideo2，还是自监督的V-JEPA，现有视频基础模型在至少一项能力上存在不足。作为替代方案，我们推出了TWLV-I，一种新型视频基础模型，能够为基于运动和外观的视频构建强健的视觉表示。根据在五个动作识别基准上进行的线性探测平均Top-1准确率，仅利用公开数据集进行预训练，TWLV-I相较于V-JEPA（ViT-L）提升了4.6%，相较于UMT（ViT-L）提升了7.7%。即使在对比更大规模的模型时，TWLV-I也表现出色，相较于DFN（ViT-H）提升了7.2%，相较于V-JEPA（ViT-H）提升了2.7%，以及相较于InternVideo2（ViT-g）提升了2.8%。此外，我们还提供了TWLV-I从多个常用视频基准中提取的嵌入向量，以及可直接利用这些嵌入向量的评估源代码，代码已公开于“https://github.com/twelvelabs-io/video-embeddings-evaluation-framework”。



## LLM Pruning and Distillation in Practice: The Minitron Approach
[LLM 剪枝与蒸馏实践：Minitron 方法](https://arxiv.org/abs/2408.11796)
我们提供了一份全面报告，介绍如何使用剪枝和蒸馏技术，将 Llama 3.1 8B 模型压缩至 4B 参数，以及将 Mistral NeMo 12B 模型压缩至 8B 参数。我们探索了两种不同的剪枝策略：(1) 深度剪枝和 (2) 联合隐藏层/注意力/MLP（宽度）剪枝，并在 LM Evaluation Harness 的常见基准上评估了结果。随后，这些模型与 NeMo Aligner 对齐，并在指令调优版本中进行测试。这种方法从 Llama 3.1 8B 中产生了一个引人注目的 4B 模型，以及从 Mistral NeMo 12B 中产生了一个最先进的 Mistral-NeMo-Minitron-8B（简称 MN-Minitron-8B）模型。我们发现，在无法访问原始数据的情况下，对教师模型在蒸馏数据集上进行轻微微调是有益的。我们在 Hugging Face 上以宽松许可开源了我们的基础模型权重。



## Controllable Text Generation for Large Language Models: A Survey
[大语言模型可控文本生成：综述](https://arxiv.org/abs/2408.12599)
在自然语言处理 (NLP) 领域，大语言模型 (LLMs) 已展现出高质量的文本生成能力。然而，在实际应用中，LLMs 面临着日益复杂的要求。除了避免误导性或不当内容外，LLMs 还需满足特定用户需求，例如模仿特定写作风格或生成富有诗意的文本。这些多样化的需求推动了可控文本生成 (CTG) 技术的发展，确保输出符合预定义的控制条件，如安全性、情感、主题一致性和语言风格，同时保持高水平的实用性、流畅性和多样性。
  本文系统地回顾了 LLMs 中 CTG 的最新进展，全面定义了其核心概念，并阐明了控制条件和文本质量的要求。我们将 CTG 任务分为两大类：内容控制和属性控制。讨论了关键方法，包括模型重训练、微调、强化学习、提示工程、潜在空间操作和解码时干预。我们分析了每种方法的特点、优势和局限性，为实现生成控制提供了细致的见解。此外，我们回顾了 CTG 评估方法，总结了其在各领域的应用，并指出了当前研究中的关键挑战，包括流畅性和实用性的降低。我们还提出了若干建议，例如在未来的研究中更加重视实际应用。本文旨在为该领域的研究人员和开发者提供有价值的指导。我们的参考列表和中文版本已在 [GitHub](https://github.com/IAAR-Shanghai/CTGSurvey) 上开源。



## Sapiens: Foundation for Human Vision Models
[Sapiens: 人类视觉模型的基础](https://arxiv.org/abs/2408.12569)
我们提出 Sapiens，一系列针对四个基本以人为中心视觉任务的模型：2D 姿态估计、身体部位分割、深度估计和表面法线预测。Sapiens 原生支持 1K 高分辨率推理，并通过在超过 3 亿张野外人类图像上预训练的模型进行简单微调，极易适应各任务。在相同计算预算下，使用精选人类图像数据集进行自监督预训练显著提升了一系列以人为中心任务的性能。这些模型在野外数据上表现出卓越的泛化能力，即使在标签数据稀缺或完全合成的情况下亦是如此。我们的模型设计简洁，随着参数从 0.3 亿扩展到 20 亿，跨任务性能不断提升。Sapiens 在多个以人为中心的基准测试中持续超越现有基线。具体来说，在 Humans-5K（姿态）上提升了 7.6 mAP，在 Humans-2K（部分分割）上提升了 17.1 mIoU，在 Hi4D（深度）上相对 RMSE 提升了 22.4%，在 THuman2（法线）上相对角度误差提升了 53.5%。


